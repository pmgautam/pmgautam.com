[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog by Pramesh Gautam",
    "section": "",
    "text": "Class Activation Map\n\n\n\n\n\n\n\ncomputer-vision\n\n\n\n\nVisual explanation to Class Activation Map using ResNet-50 in PyTorch\n\n\n\n\n\n\nAug 5, 2022\n\n\nPramesh Gautam\n\n\n\n\n\n\n  \n\n\n\n\nAugmentations: Cutout, Mixup, Cutmix, Label Smoothing\n\n\n\n\n\n\n\ncomputer-vision\n\n\naugmentations\n\n\n\n\nVisual summary and implementation of augmentations techniques: Cutout, Mixup, Cutmix and Label Smoothing\n\n\n\n\n\n\nMar 27, 2022\n\n\nPramesh Gautam\n\n\n\n\n\n\n  \n\n\n\n\nKnowledge Distillation explained\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndistillation\n\n\n\n\nExplanation of Knowledge Distillation in PyTorch along with implementation\n\n\n\n\n\n\nOct 3, 2021\n\n\nPramesh Gautam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/augmentations-visually-explained.html",
    "href": "posts/augmentations-visually-explained.html",
    "title": "Augmentations: Cutout, Mixup, Cutmix, Label Smoothing",
    "section": "",
    "text": "Data augmentation is one of the major techniques used when we want to improve the model’s performance on new data (i.e. generalization). It is especially useful in cases where only limited training data are available. In this post we’ll discuss some of the augmentation techniques (besides affine transformation) that help the models to perform well (in computer vision).\n\n\nCode\n# install required libraries\n# !pip install opencv-python==4.5.1.48\n# !pip install numpy==1.21.0\n# !pip install matplotlib==3.4.2\n\n\n\n\nCode\n# import the required libraries\nfrom copy import deepcopy\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# set random seed to reproduce results\nnp.random.seed(42)\n\n\n\n\nCode\n# define labels and read images\n\ncat_label = np.array([[1, 0]])\ndog_label = np.array([[0, 1]])\n\ncat_img = cv2.imread(\"images/cat.jpg\")/255.0\ndog_img = cv2.imread(\"images/dog.jpg\")/255.0\n\n\n\n\nCode\n# plot the original images\n\nfig, ((ax1, ax2)) = plt.subplots(1, 2)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nplt.suptitle(\"Original images\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\nCutout\npaper link\nThe main motivation of Cutout is to simulate the situation of object occlusion that is mostly encountered in tasks such as object recognition or human pose estimation. We occlude the part of image randomly. Instead of model seeing the same image everytime, it sees different parts of that image which helps it to perform well. The occluded part does not contain any information. In the example below, the randomly occluded part is replaced by all 0s.\n\n\nCode\nclass Cutout(object):\n    def __init__(self, cutout_width):\n        # width of the square to occlude\n        self.cutout_width = cutout_width\n\n    def __call__(self, img):\n        h = img.shape[0]\n        w = img.shape[1]\n\n        mask = np.ones(img.shape)\n\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        x1 = np.clip(x - self.cutout_width // 2, 0, w)\n        x2 = np.clip(x + self.cutout_width // 2, 0, w)\n\n        y1 = np.clip(y - self.cutout_width // 2, 0, h)\n        y2 = np.clip(y + self.cutout_width // 2, 0, h)\n\n        mask[y1: y2, x1: x2] = 0.0\n\n        # occlude the part of image using mask (zero out the part)\n        img = img * mask\n\n        return img\n\n\n\n\nCode\ncutout = Cutout(50)\nplt.imshow(cutout(cat_img))\nplt.suptitle(\"Cutout example\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\nMixup\npaper link\nMixup linearly mixes two images and their labels in the same proportion. With this approach the model will be able to see a different image with different label during training. It enables the model to make smooth decision boundaries while classifying the object since we are using linearly interpolated images and labels for classification decision intead of binary decision. As can be seen in the example below, with the mixup of images and labels, the new image will have the labels [0.19, 0.81] which means the class distribution is tilted more towards dog and the mixup visualization also proves this.\n\n\nCode\nclass Mixup:\n    def __init__(self, img1, img2, label1, label2):\n        self.img1 = img1\n        self.img2 = img2\n        self.label1 = label1\n        self.label2 = label2\n    \n    def __call__(self):\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        \n        # mix image and labels        \n        mix_img = lam * self.img1 + (1 - lam) * self.img2\n        mix_label = lam * self.label1 + (1 - lam) * self.label2\n        \n        return mix_img, mix_label\n\n\n\n\nCode\nmix_img, mix_label = Mixup(cat_img, dog_img, cat_label, dog_label)()\n\n\n\n\nCode\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(mix_img)\nax3.set_title(\"Cat/Dog Mixup\")\nplt.suptitle(\"Mixup example\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\nCutmix\npaper link\nAs we saw above, the occluded part in Cutout does not contain any information which in unwanted since it just increases resource usage without adding any value. Cutmix utilizes above two techniques to mix the images and utilize the occluded part. As can be seen in the example, the cutout portion in dog image is replaced by the same portion of cat image. It also linearly interpolates the label as in Mixup. As seen in the example below, the new labels are [0.18, 0.82] for cat and dog respectively and this can be verified from image as well.\n\n\nCode\nclass Cutmix:\n    def __init__(self, img1, img2, lbl1, lbl2):\n        self.img1 = img1\n        self.img2 = img2\n        self.lbl1 = lbl1\n        self.lbl2 = lbl2\n    \n    def __call__(self):\n        # sample bounding box B\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        h, w, c = self.img1.shape\n        \n        r_x = np.random.randint(0, w)\n        r_y = np.random.randint(0, h)\n        r_w = np.int32(w * np.sqrt(1 - lam))\n        r_h = np.int32(h * np.sqrt(1 - lam))\n        \n        mid_x, mid_y = r_w//2, r_h//2\n        \n        point1_x = np.clip(r_x - mid_x, 0, w)\n        point2_x = np.clip(r_x + mid_x, 0, w)\n        \n        point1_y = np.clip(r_y - mid_y, 0, h)\n        point2_y = np.clip(r_y + mid_y, 0, h)\n        \n        # refer to paper to see how M is cimputed, in short it is to make the ratio of mixing images and labels same\n        M = np.ones(self.img1.shape)\n        M[point1_x:point2_x, point1_y:point2_y, :] = 0\n        \n        cut_mix_img = M * self.img1 + (1 - M) * self.img2\n        cut_mix_label = lam * self.lbl1 + (1 - lam) * self.lbl2\n        \n        return cut_mix_img, cut_mix_label\n\n\n\n\nCode\ncut_mix_img, cut_mix_lbl = Cutmix(cat_img, dog_img, cat_label, dog_label)()\n\n\n\n\nCode\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(cut_mix_img)\nax3.set_title(\"Cat mixed onto Dog\")\nplt.suptitle(\"Cutmix example\", fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\nLabel Smoothing\npaper link\nLabel smoothing is used so that the model can be prevented from memorizing the training data and being over-confident. It adds noise to the labels without modifying the data itself. If we are using a 5-class classifier then instead of label being assigned only to one class, it is distributed among all the classes. As seen in example below, \\(\\epsilon\\) is used to control the smoothing.\n\n\nCode\nϵ = 0.01\nnum_labels = 5\nclasses = [\"cat\", \"dog\", \"horse\", \"bus\", \"carrot\"]\n\noriginal_label = [1, 0, 0, 0, 0]\nnew_neg_labels = ϵ/(num_labels-1)\n\n# after label smoothing, cat gets 1 - ϵ and other classes get ϵ/(1-ϵ) probability\nsmooth_labels = [1 - ϵ, new_neg_labels, new_neg_labels, new_neg_labels, new_neg_labels]\n\nsmooth_labels\n\n\n[0.99, 0.0025, 0.0025, 0.0025, 0.0025]\n\n\nAll the above implementations assume that both the images are of same resolution. There might be some minor differences while compared with the original paper. However the main motivation and results of these techniques are same to that of paper. Please feel free to post any comment, question or suggestion. I’ll see you in the next one. :)"
  },
  {
    "objectID": "posts/class-activation-map-explained.html",
    "href": "posts/class-activation-map-explained.html",
    "title": "Class Activation Map",
    "section": "",
    "text": "Code\n# imports\n\nimport ast\nimport json\nfrom copy import deepcopy\n\nimport imageio\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models, transforms"
  },
  {
    "objectID": "posts/class-activation-map-explained.html#pytorch-hooks",
    "href": "posts/class-activation-map-explained.html#pytorch-hooks",
    "title": "Class Activation Map",
    "section": "Pytorch Hooks",
    "text": "Pytorch Hooks\nWe’ll be using PyTorch hooks to extract the intermediate feature maps. Hooks are the functions that can be executed during forward or backward pass of the network. You can learn model about hooks here.\nHere we are using hooks to save the feature maps of layer4, i.e. the final convolutional layer. We multiply the output of layer4 by the weights that map from avgpool layer to fc layer. Since weights can be extracted from the model itself, we only use hooks to save the output of laster convolutional layer.\n\n\nCode\n# define hooks to save activation\nactivation = {}\n\ndef get_activation(name):\n    def hook(model, input, output):        \n        activation[name] = output.detach()\n    return hook\n\n\nOnce we define attach hook to the model, the output of the layer to which the hooks was attached will be saved in the activation dictionary."
  },
  {
    "objectID": "posts/class-activation-map-explained.html#implementation",
    "href": "posts/class-activation-map-explained.html#implementation",
    "title": "Class Activation Map",
    "section": "Implementation",
    "text": "Implementation\n\n\nCode\n# define function to get predictions and required activations\n\ndef get_data(input_image, model, transforms):\n    # transform input image\n    input_data = transform(imageio.imread(input_image)).unsqueeze(0)\n    \n    # imagenet labels to map from index to text label\n    labels = ast.literal_eval(open(\"imagenet1000_clsidx_to_labels.txt\").read())\n    \n    # attach hook to layer 4\n    model.layer4.register_forward_hook(get_activation(\"layer4\"))\n    \n    model.eval();\n    with torch.no_grad():\n        preds = model(input_data)\n        preds_softmax = torch.nn.functional.softmax(preds, dim=1)\n        top_prob, top_pred = preds_softmax.max(dim=1)\n\n        return top_pred, top_prob, labels[top_pred.item()], activation\n\n\n\n\nCode\n# image to test CAM\nimage = \"images/n02102040_7490.jpeg\"\n\n# model to run inference\nmodel = models.resnet50(pretrained=True)\n\n# transforms to use ImageNet values for normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# results\npred_class, pred_prob, pred_label, activation = get_data(image, model, transforms)\n\npred_class, pred_prob, pred_label\n\n\n(tensor([217]), tensor([0.8239]), 'English springer, English springer spaniel')\n\n\n\n\nCode\n# input image\ninput_img = imageio.imread(image)\nplt.imshow(input_img);\n\n\n\n\n\n\n\nCode\n# extract weights from FC layer that correspond to the predicted class\n\nfc_weights = model.fc.weight[pred_class, :].unsqueeze(2).unsqueeze(3)\nfc_weights.shape\n\n\ntorch.Size([1, 2048, 1, 1])\n\n\n\n\nCode\n# check shape of final convolutional layer\nactivation[\"layer4\"].shape\n\n\ntorch.Size([1, 2048, 10, 14])\n\n\n\n\nCode\n# weighted summation of final feature maps\ncam = (fc_weights * activation[\"layer4\"]).sum(1, keepdim=True)\n\n# add dimension to make compatible with interpolation\ncam = cam\ncam.shape\n\n\ntorch.Size([1, 1, 10, 14])\n\n\n\n\nCode\n# reshape cam to original input shape\nfinal_cam = F.interpolate(cam, input_img.shape[:2], mode=\"bilinear\")\n\n\n/home/pramesh/miniconda3/envs/py3.7/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning:\n\nDefault upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n\n\n\n\n\nCode\n# plot input image with CAM overlayed\nplt.imshow(input_img)\nplt.imshow(final_cam.squeeze().detach().numpy(), alpha=0.8)\n\nplt.show()\n\n\n\n\n\nAs can be seen in the plot above, the network is able to focus on the dogs while classification. We can weight the features first and then resize the image or we can resize the features first and use the weighted combination. The results are identical. Below I have first resized the final feature maps and then used their weighted combination for plotting.\n\n\nCode\n# resizing the feature maps and then weighting\nfeatures = F.interpolate(activation[\"layer4\"], tuple(input_img.shape[:2]), mode=\"bilinear\")\n\ncam_new = (fc_weights * features).sum(1, keepdim=True)\ncam_new.shape\n\n\ntorch.Size([1, 1, 320, 426])\n\n\n\n\nCode\nplt.imshow(input_img)\nplt.imshow(cam_new.squeeze().detach().numpy(), alpha=0.8);\n\n\n\n\n\nThank you for going through the post and I hope is was helpful to understand Class Activation Maps. I will add Grad-CAM in future post. Please post comments if anything is not clear or if you have any suggestions.\nBye :)"
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html",
    "href": "posts/knowledge-Distillation-explained.html",
    "title": "Knowledge Distillation explained",
    "section": "",
    "text": "In this post I’ll be discussing about Knowledge Distillation. Basically, I’ll be summarizing the concept from this paper on Knowledge Distillation. Knowledge Distillation is the process of training a smaller network by using the concepts/knowledge that has been learned by the large network. Basically, it is the process of distilling knowledge from one model to another."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#motivation",
    "href": "posts/knowledge-Distillation-explained.html#motivation",
    "title": "Knowledge Distillation explained",
    "section": "Motivation",
    "text": "Motivation\nLarger models have high learning capacity and can perform well when trained on large datasets. This is the pattern followed in Machine Learning. If getting the best accuracy is the only aim then this approach is fine. But when we want to deploy the model in hardware with less compute or time constraints (e.g: deploying models on mobile phones), deploying large models is not an option. We want to have smaller models that perform on par or close to large models but be efficient in computation. The main motivation behind Knowledge Distillation is to use large, complex models during training so that they can extract a lot of concepts from the data and use these models to train smaller models that will be used in inference as they are more efficient (computation and memory-wise) than large models."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#analogy",
    "href": "posts/knowledge-Distillation-explained.html#analogy",
    "title": "Knowledge Distillation explained",
    "section": "Analogy",
    "text": "Analogy\nAn analogy from stages of butterfly’s life can be related to the concepts of Knowledge Distillation. They need to eat a lot to gain nutrition and energy. They also need to be light to fly around and mate. Since these are opposite tasks, there are separate forms for such tasks. Caterpillar is the stage that feeds a lot to gain energy and nutrition. Its task is just eating. Once this stage is complete, it is transformed to butterfly for tasks such as flying, mating that require lightweight. The same pattern can be suitable in Machine Learning as discussed in motivation (training a large complicated model and using a small/light model for inference)."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#task",
    "href": "posts/knowledge-Distillation-explained.html#task",
    "title": "Knowledge Distillation explained",
    "section": "Task",
    "text": "Task\nThe task in Knowledge Distillation is to train a smaller model that performs better than itself trained from scratch. For this, a large model using lots of data to high accuracy will be trained first. This is known as a teacher. It is able to learn a lot of concepts with its large size/learning capacity. For inference, a smaller model is trained using the knowledge acquired by the large model. The small model that is used in inference is called student."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#knowledge",
    "href": "posts/knowledge-Distillation-explained.html#knowledge",
    "title": "Knowledge Distillation explained",
    "section": "Knowledge",
    "text": "Knowledge\n\n\n\n\n\n\n\n\n\nInput image\n\n\n\n\n\n\n\nPredicted class\nCar\nTruck\nCarrot\n\n\nConfidence\n0.9\n0.09\n0.01\n\n\n\nKnowledge is the concept that is acquired by the teacher model, i.e: output of teacher for various images. In the above example, although the model is quite sure that the image is of car, there is something interesting about probabilities of other classes as well. We can see that the model is far more sure about the image being a truck than a carrot. This knowledge is used by Knowledge Distillation to train a smaller model. The model having 0.09 confidence of being a truck and 0.01 confidence of being a carrot is a very useful information. We ignore this knowledge while training a classifier. Knowledge distillation aims to use this knowledge effectively to train the smaller model. It is also called dark knowledge as it exists in the model but is not used for downstream task. Knowledge distillation utilizes this knowledge."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#approach",
    "href": "posts/knowledge-Distillation-explained.html#approach",
    "title": "Knowledge Distillation explained",
    "section": "Approach",
    "text": "Approach\n\nSoft targets\n\n\n\n\n\n\n\n\n\nTarget Type (\\(\\downarrow\\)) Class (\\(\\rightarrow\\))\nCar\nTruck\nCarrot\n\n\n\n\nHard targets\n1\n0\n0\n\n\nModel outputs\n0.9\n0.09\n0.01\n\n\nSoft targets\n0.8\n0.15\n0.05\n\n\n\nAs seen in the above table, there are three types of targets that denote the class of a given image. Hard targets denote the class to which the image belongs using 1 and the rest of the classes have a value 0. This is also called one-hot encoding. The second row is the example of model outputs without any changes. In the case of multi-class classification, it is the value of softmax that outputs softer distribution than hard targets and all the classes are assigned some probability values. The last row denotes soft targets which are softened by using the temperature parameter in softmax function. As we make the distribution softer and softer, high probabilities will decrease and small probabilities will increase. Making the distribution softer can make the knowledge valuable to student as it can carry the concepts more clearly (in this case the probabilities of each class i.e: model gives the information not just about the image that it is most sure of but regarding other classes as well). In this example, the model can provide information that there are details that are related to truck and carrot (although smaller) which can be a valuable information while teaching the student model.\n\nSoftmax function\n\\[q_i = \\frac{exp(z_i)}{\\sum_{j}exp(z_j)}\\]\n\n\nSoftmax function with temperature parameter\n\\[q_i = \\frac{exp(z_i/\\tau)}{\\sum_{j}exp(z_j/\\tau)}\\]\n\\(\\tau\\) is called temperature. It controls the extent to which the distribution is to be softened and can be decided by using hyperparameter search.\nIn PyTorch this can be computed as:\nimport torch\nimport torch.nn as nn\n\nlogits = torch.tensor([2.8, 0.1, -1]).reshape(1, -1) #output of model for car, truck and carrot\nt = 1 #temperature parameter to make distribution soft\nsoftmax = nn.Softmax(dim=1)\n\nsoftmax(logits/t)\n\n# Outputs\ntensor([[0.9178, 0.0617, 0.0205]]) # t=1\ntensor([[0.7098, 0.1840, 0.1062]]) # t=2\ntensor([[0.5923, 0.2408, 0.1669]]) # t=3\ntensor([[0.4877, 0.2842, 0.2281]]) # t=5\nAs we increase the temperature parameter the softmax output changes to a softer distribution."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#training-teacher",
    "href": "posts/knowledge-Distillation-explained.html#training-teacher",
    "title": "Knowledge Distillation explained",
    "section": "Training teacher",
    "text": "Training teacher\nTraining a teacher is similar to training other neural networks. Since the objective is to learn as much as possible so that the student could be taught using the knowledge learned by the teacher, normally large model is trained on large datasets. Teacher can also be an ensemble of models."
  },
  {
    "objectID": "posts/knowledge-Distillation-explained.html#training-student",
    "href": "posts/knowledge-Distillation-explained.html#training-student",
    "title": "Knowledge Distillation explained",
    "section": "Training student",
    "text": "Training student\nTraining a student is the main contribution of Knowledge Distillation. Student is trained using the knowledge gathered by teacher as well as the ground truth labels.\n\\[p^\\tau_T = \\text{softmax}(a_T/\\tau)\\]\n\\[p^\\tau_S = \\text{softmax}(a_S/\\tau)\\]\n\\[hard\\_loss = CE(y_{true}, p_S)\\]\n\\[soft\\_loss = KL(p^\\tau_T, p^\\tau_S)\\]\n\\[KD\\_loss = (1 - \\lambda) * soft\\_loss + \\lambda * hard\\_loss\\]\n\\(\\lambda\\) is the weight hyperparameter, \\(CE\\) and \\(KL\\) denote Crossentropy and KL divergence respectively.\nWe can train the student by minimizing KD_loss. As seen from the equation, Knowledge distillation uses both the hard labels and soft labels to train the student model.\n\nAs seen in the diagram above, student tries to match the soft targets from the teacher."
  }
]