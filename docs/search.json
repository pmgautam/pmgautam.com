[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Email: prms&lt;last_name&gt; [at] gmail [dot] com\n\nInterested in:\n\nMachine Learning/Deep Learning\nGen AI\nData\nSoftware Engineering"
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html",
    "href": "posts/lessons-learned-building-genai-apps.html",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "",
    "text": "We’ve pivoted from an era of “train your own model” to one of “call an API.” On the surface, this shift is incredibly empowering. The barrier to entry has never been lower. However while it is easier than ever to build a prototype, the complexity of shipping a reliable, production-ready system is higher than it’s ever been.\nAfter spending the last two years building GenAI applications, I’ve seen the good, the bad, and the buggy. I’m sharing these practical lessons not as an expert with all the answers, but as an engineer passing along my notes. Let’s get into the lessons I’ve learned so far.\nThis is by no means a complete receipe. You might feel that you do not agree with some of the points and that is totally fine. These are the lessons that I’ve learned and hope they’ll be helpful to you in one way or the other."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#introduction",
    "href": "posts/lessons-learned-building-genai-apps.html#introduction",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "",
    "text": "We’ve pivoted from an era of “train your own model” to one of “call an API.” On the surface, this shift is incredibly empowering. The barrier to entry has never been lower. However while it is easier than ever to build a prototype, the complexity of shipping a reliable, production-ready system is higher than it’s ever been.\nAfter spending the last two years building GenAI applications, I’ve seen the good, the bad, and the buggy. I’m sharing these practical lessons not as an expert with all the answers, but as an engineer passing along my notes. Let’s get into the lessons I’ve learned so far.\nThis is by no means a complete receipe. You might feel that you do not agree with some of the points and that is totally fine. These are the lessons that I’ve learned and hope they’ll be helpful to you in one way or the other."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#understand-the-task",
    "href": "posts/lessons-learned-building-genai-apps.html#understand-the-task",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "1. Understand the Task",
    "text": "1. Understand the Task\nBefore writing a single line of code, you must deeply understand what you are actually trying to solve. This might sound like “Software Engineering 101,” but in the world of LLM-driven apps, it is where most projects fail.\nMisunderstanding the task is like having a heavy-duty foundation for a commercial warehouse when the client actually needed a residential cottage. You might complete the build on time and under budget, but the end result is useless to the person living there.\nDefining the task isn’t just about transcribing a client’s wish list. It’s about user empathy - You have to put yourself in the end-user’s shoes: - What do they actually expect when they click “Submit”? - If they ask for a “summarizer,” are they looking for a three-sentence executive blurb or a technical list of action items? - What information is “signal” to them, and what is just “noise” they want excluded?\nYou likely won’t get every detail right on the first try, and that’s okay. However, taking the time to truly deconstruct the task helps you plan your architecture effectively and, more importantly, manage the client’s expectations from day one."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#know-the-datadomain",
    "href": "posts/lessons-learned-building-genai-apps.html#know-the-datadomain",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "2. Know the Data/Domain",
    "text": "2. Know the Data/Domain\nOnce the task is defined, your next challenge is mastering the data and the domain. In my experience, this is often where the most significant project friction occurs.\nThere is a natural “communication gap” here: clients are usually domain experts who are non-technical, while engineers are technical experts who are rarely familiar with the nuances of healthcare, finance, or law. If you don’t bridge this gap early, you risk heading down a “one-way road” that is incredibly expensive to return from.\nIn a healthcare project I worked on, the medical data was so specialized that we initially struggled to make progress. Our solution? We scheduled daily syncs with the client just to decode their terminology and workflows. While this felt like a poor use of time at first, it ended up being our smartest investment. It ensured we built on a foundation of facts rather than assumptions, saving us months of potential rework.\nStrategies for mastering the domain:\n\nBuild a Glossary: Learn industry-specific terms and exactly how the client defines them.\nAudit Data Quality Early: The “Garbage In, Garbage Out” rule is true for LLMs as well. If your data is messy, your model’s reasoning will be too.\nIdentify Sensitive Content: Flag PII (Personally Identifiable Information) and regulated data (like HIPAA or GDPR) before a single token is sent to an API.\nIdentify Edge Cases: Ask the client: _“What is a scenario that happens only 1% of the time, but would be a disaster if the AI got it wrong?”\nReverse-Engineer the Human Workflow: How do humans currently solve this problem? If you can’t map the human process, you’ll struggle to automate it reliably."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#define-evals-early",
    "href": "posts/lessons-learned-building-genai-apps.html#define-evals-early",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "3. Define Evals Early",
    "text": "3. Define Evals Early\n\n“You can’t improve what you don’t measure.”\n\nThis is arguably the most critical pillar of GenAI development. Without a robust evaluation framework, you are essentially operating without a clear way to system improvement. You might write a prompt, test it against five cases, and conclude that it “works.” But in production, LLMs fail in silent, creative ways that a handful of manual checks will never catch.\nEvaluation is the foundation of the GenAI lifecycle:\n\nBuild “Eval Sets” before Production Code: Treat your evaluation dataset as your “Ground Truth.” It should contain diverse examples, from the standard “happy path” to the weirdest edge cases you identified in Section 2.\nThe 2025 Metric Mix: Don’t rely solely on old-school metrics like BLEU or ROUGE. They cannot capture the meaning. Use a hybrid approach:\n\nDeterministic Checks: For formatting (e.g., “Is the output valid JSON?”).\nSemantic Similarity: Using embedding distance to check if the meaning matches your target, even if the words differ.\nLLM-as-a-Judge: Use a “frontier” model (like GPT-5 or Claude 4.5 Sonnet) to grade your system’s performance based on specific rubrics like faithfulness or helpfulness.\nTask specific metrics: Never forget to use task-specific metrics. Even though you use LLM for classification, using F1 score is almost always a good idea than using LLM as a judge. The only downside is you need to prepare ground truth.\n\nHuman-in-the-Loop (HITL): This remains the gold standard. Use human experts to periodically audit the automated scores. If the automated eval says “Pass” but a human says “Fail,” your eval criteria need tuning.\nMake Evals Fast and Versioned: You will run these hundreds of times. If your eval takes an hour, you won’t run it. Aim for a “smoke test” suite that runs in minutes and track how your scores change as you update prompts.\nRelentless Error Analysis: Don’t just settle for a score (e.g., “80% accuracy”). Dive into the failures. Is the model hallucinating? Is it a formatting issue? Is the retrieved data wrong? It’s where you discover exactly what to optimize next."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#embrace-observability",
    "href": "posts/lessons-learned-building-genai-apps.html#embrace-observability",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "4. Embrace Observability",
    "text": "4. Embrace Observability\nOnce you move from a local notebook to a production environment, monitoring is no longer a “nice-to-have”—it is your lifeline. Traditional software monitoring focuses on “uptime” (is the server alive?), but GenAI monitoring must track intelligence (is the answer correct, fast, and affordable?).\nWithout proper observability, LLMs are a black box. You won’t know if a sudden spike in user complaints is due to a “silent” model update from OpenAI, a “data drift” in your vector database, or an inefficient prompt chain. LLM Observability tools like Langfuse (great for open-source flexibility) and LangSmith (if you are already in the LangChain ecosystem) are essential for peering inside this box.\nYour observability strategy should focus on these three pillars:\n\n1. Deep Request Tracing: In a complex app (like a RAG pipeline), a single user question triggers multiple events: a search, a context retrieval, and finally an LLM call. Tracing allows you to map this entire lifecycle. If the final answer is wrong, you can pinpoint exactly where the “logic chain” broke—did the retriever find the wrong document, or did the LLM just ignore the right one?\n2. Performance & Unit Economics: Beyond total latency, you need to track TTFT (Time to First Token). In 2025, users care more about how quickly the AI starts responding than when it finishes. Simultaneously, you must monitor per-user token consumption; a small bug in a loop can “burn” thousands of dollars in hours if you aren’t accounting for your token spend in real-time.\n3. Quality & Drift Detection: Models aren’t static. Providers update them “under the hood,” which can lead to silent failures—where the API returns a 200 OK but the response is gibberish or non-compliant. Continuous oversight helps you catch these “personality shifts” and hallucinations before they erode user trust."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#take-clients-onboard",
    "href": "posts/lessons-learned-building-genai-apps.html#take-clients-onboard",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "5. Take Clients Onboard",
    "text": "5. Take Clients Onboard\nWhile you are busy understanding the task and auditing the data, you must ensure you are communicating clearly with your stakeholders. It is your job to manage the “AI hype.” You need to be transparent about what the system can do, what its limitations are, and how you plan to improve it over time—especially if the task is complex and cannot be fully solved in the initial phases.\nKey strategies for client alignment:\n\nBe explicit about capabilities: Clearly define what the AI can and cannot do. Don’t let them assume it has “human-level” reasoning if it’s just a classification tool.\nShow failure cases upfront: Don’t just demo the “happy path.” Demonstrate limitations and edge cases early so there are no surprises during production.\nDefine responsibility boundaries: Establish where the AI’s job ends and where human oversight must begin. This is crucial for maintaining quality and trust.\nEducate on the probabilistic nature: Explain that LLMs are not deterministic. Unlike traditional software, the outputs won’t always be perfect or 100% consistent, and the client needs to be prepared for that variability.\nAgree on iteration cycles: Make it clear that GenAI development is experimental by nature. Success requires a cycle of testing, feedback, and refinement rather than a single “final” release."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#cost-considerations-from-day-1",
    "href": "posts/lessons-learned-building-genai-apps.html#cost-considerations-from-day-1",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "6. Cost Considerations from day 1",
    "text": "6. Cost Considerations from day 1\nThere is a high probability that costs will catch you by surprise if they aren’t factored into your architecture from day one. It’s easy to build a high-performing PoC (Proof of Concept) using a five-page document, but production reality is different. When that same system suddenly has to process hundreds of 50-page documents, your servers won’t crash—but your LLM bill certainly might.\nAs an engineer, it is your responsibility to set clear boundaries regarding the nature of the data and the expected costs. If a client is fascinated by a complex multi-chain agent that you built for a demo, you must help them understand the “unit economics” of that solution.\nTips for cost-effective scaling:\n\nEstimate Early and Often: Don’t wait for the first invoice. Calculate the expected cost per user and cost per request during the design phase.\nThe “Smaller Model” First Rule: If a smaller, faster model (like GPT-4o-mini or Claude Haiku) can achieve 90% of the result, use it. It optimizes both latency and cost simultaneously.\nManage Multi-Chain Workflows: Every step in an autonomous chain adds to the bill. Be transparent with stakeholders about the price of complexity.\nDefine Data Boundaries: Set limits on document sizes or the number of documents a user can upload to prevent costs."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#prioritize-the-feedback-loop",
    "href": "posts/lessons-learned-building-genai-apps.html#prioritize-the-feedback-loop",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "7. Prioritize the Feedback Loop",
    "text": "7. Prioritize the Feedback Loop\nOnce you release the system to UAT (User Acceptance Testing) or production and real users start interacting with it, your most valuable asset is their feedback. Many teams treat feedback as an afterthought, but implementing a mechanism to capture user sentiment in the early stages is one of the smartest moves you can make.\nThis feedback doesn’t just tell you if the system “works”, it builds a unique, high-value dataset that reveals exactly where the model’s responses fall short of user expectations. It is the ultimate tool for pressure-testing your initial assumptions.\nHow to build an effective feedback loop?\n\nMake it Frictionless: Use a simple one-click “Thumbs Up/Down” interface. If the barrier to giving feedback is too high, users simply won’t do it.\nCapture Structured Data: When a user gives negative feedback, ask a quick follow-up question: “Was the answer inaccurate, too long, or poorly formatted?” This helps you categorize issues without manual review.\nAnalyze the Patterns: Look for clusters. Are users consistently complaining about the same feature? This is your signal to revisit your implementation.\nTurn Feedback into Evals: Take the cases where users were dissatisfied and turn them into permanent test cases in your evaluation suite. This ensures you never regress on the same issue twice.\nClose the Loop: Show your users that their input matters. Improving the system based on their direct feedback is the fastest way to build trust in an AI product."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#embrace-the-experimental-nature",
    "href": "posts/lessons-learned-building-genai-apps.html#embrace-the-experimental-nature",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "8. Embrace the Experimental Nature",
    "text": "8. Embrace the Experimental Nature\nStakeholders are often “wowed” by the initial PoC, only to become frustrated when they realize how many ways the system can fail in the wild. As an engineer, your job is to set the expectation that GenAI development is not a “one-and-done” task. It is a continuous, iterative process.\nA production system is a moving target: prompt performance shifts, model providers push updates, the nature of incoming data evolves, and user feedback will constantly pull the project in new directions. All of these factors force you into a state of perpetual experimentation. To make your case to leadership, don’t just ask for more time; present a structured plan for iteration. One of the most effective habits is to align your experimentation with your feedback loops.\n\nExample: The “Prompt Release” Strategy\nIn one project, we realized we needed to update our prompts constantly based on evolving client needs. To manage this without causing chaos, we treated prompt updates with the same discipline as code deployments. We established a dedicated “Prompt Release Day.”\nOur weekly cycle looked like this:\n\nFriday: Deadline for receiving the “First Draft” of requirements or feedback from the client.\nMonday – Wednesday: Dedicated time for experimentation, testing new prompt versions, and running them through our Eval sets.\nThursday: Release Day. The new, tested prompts are pushed to production.\n\nThis process did two things: it gave the engineers a predictable window to experiment, and it gave the clients a reliable release cycle for when they would see improvements. It turned “unpredictable AI magic” into a standard, manageable engineering workflow."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#treat-prompts-as-code",
    "href": "posts/lessons-learned-building-genai-apps.html#treat-prompts-as-code",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "9. Treat Prompts as Code",
    "text": "9. Treat Prompts as Code\nBuilding on the idea of a release cycle, the biggest mindset shift you need is this: Treat your prompts with the same rigor as your application code. If your prompts are scattered across random Jupyter notebooks or buried as hard-coded strings deep inside your functions, you are setting yourself up for a maintenance nightmare. Treating prompts as code means applying the same engineering disciplines you use for any other part of your technical stack.\nWhat “Prompts as Code” looks like in practice:\n\nVersion Control: Store your prompts in your repository, or even better, use a dedicated prompt management tool like Langfuse. This allows you to track exactly who changed what and, more importantly, enables you to roll back to a “known-good” version immediately if a new tweak causes the model’s performance to degrade.\nPrompt Reviews: Never push a major prompt change without a peer review. In GenAI, a single word change like swapping “must” for “should” can drastically alter the model’s output logic. A second pair of eyes is essential to catch these subtle shifts in behavior.\nTemplating: Don’t mix logic with data. Use structured formats to keep your system instructions separate from user inputs. While the “how-to” of prompt engineering is outside the scope of this article, keeping your prompts clean and modular is a fundamental part of building a manageable system.\nAutomated Testing: Just as you wouldn’t ship a function without a unit test, don’t ship a prompt without running it against your Eval set. This testing should be multi-dimensional, covering everything from expected output quality to security checks (like prompt injection)."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#frameworks-might-be-overkill",
    "href": "posts/lessons-learned-building-genai-apps.html#frameworks-might-be-overkill",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "10. Frameworks Might Be Overkill",
    "text": "10. Frameworks Might Be Overkill\nI have seen many teams rush to adopt frameworks like LangChain or LlamaIndex the moment they start a project. In the early prototyping stages, these tools feel like magic—they make complex tasks look easy with just a few lines of code and standard implementations.\nHowever, as your system grows and your LLM chains become more complex, you may realize that these frameworks add unwanted complexities to the project.\nBy design, they abstract away much of the underlying logic. While this saves time initially, it eventually becomes a bottleneck. You often end up “blind” to the internal prompts being used, and simple customizations can become incredibly tricky. I’ve seen engineers spend more time fighting a framework’s codebase or trying to override its defaults than they would have spent writing the actual business logic from scratch.\nThe “Simple-First” Approach: It is often better to start with simple, direct API calls and focus on structuring your own codebase properly from day one. This is where your traditional software engineering skills really come to the rescue. Building your own thin wrappers around LLM calls gives you:\n\nTotal Transparency: You know exactly what prompt is being sent and why.\nEasier Debugging: There are no hidden “black-box” layers between your code and the model.\nLong-term Maintainability: You aren’t at the mercy of a framework’s breaking changes or opinionated updates.\n\nDon’t let the hype around “agentic frameworks” distract you. Sometimes, a well-organized Python script beats a complex library."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#error-analysis-is-precious",
    "href": "posts/lessons-learned-building-genai-apps.html#error-analysis-is-precious",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "11. Error Analysis is Precious",
    "text": "11. Error Analysis is Precious\nOnce your evals are running, they will provide a performance score but a score alone doesn’t tell you how to improve. You need to look under the hood to understand exactly what kind of errors are occurring, where they are happening, and how to fix them. This is where Error Analysis becomes your most valuable tool.\nGenAI applications demand the same rigorous error analysis used in traditional Machine Learning. Instead of just seeing that the system failed, you need to categorize why it failed.\nTo make error analysis effective, categorize your failures into buckets. This sounds simple, but in practice you’ll discover new failure modes you never imagined. Here are some that I could recall.\n\nRetrieval Failures: Did the system fail because the RAG pipeline didn’t find the right information? (Fix: Optimize your embeddings or chunking).\nReasoning Failures: Did the AI have the right information but failed to connect the dots? (Fix: Improve the prompt or move to a more capable model).\nFormatting/Constraint Failures: Did the AI follow the instructions but fail to output the correct JSON or XML format? (Fix: Use few-shot examples or constrained decoding).\nHallucinations: Did the AI invent facts that weren’t in the source? (Fix: Tighten your system prompt or add “grounding” checks)."
  },
  {
    "objectID": "posts/lessons-learned-building-genai-apps.html#expectation-vs-reality-trap",
    "href": "posts/lessons-learned-building-genai-apps.html#expectation-vs-reality-trap",
    "title": "From PoC to Production: Lessons Learned Building GenAI Apps",
    "section": "12. Expectation vs Reality Trap",
    "text": "12. Expectation vs Reality Trap\nIn the beginning, everyone tries their best to build a “perfect” system. But as the project nears the finish line, both you and your stakeholders will likely face a harsh Expectation vs. Reality moment.\nA demo is a controlled environment. It works because you are using curated data, “happy path” queries, and perhaps a model that hasn’t been hit with rate limits or latency spikes yet. Production, however, is messy and unpredictable.\nWhy the gap exists:\n\nThe “Golden Example” Fallacy: Demos usually showcase the 5% of cases where the AI is brilliant. Production forces the AI to handle the other 95%—the ambiguous, the messy, and the nonsensical inputs.\nThe Creativity of Users: You cannot predict how a real user will interact with your system. They will ask questions you never tested and use the tool in ways you never intended.\nHidden Complexity: As you add guardrails, PII filters, and cost-management logic, the “clean” performance of the initial PoC can start to degrade.\n\nDon’t aim for a “perfect” launch. Aim for a resilient one. Prepare your stakeholders for the fact that the first version in the wild will reveal gaps you couldn’t see in the lab. This is why the feedback loops and experimentation cycles we discussed earlier are so critical."
  },
  {
    "objectID": "posts/openai-clip-explained.html",
    "href": "posts/openai-clip-explained.html",
    "title": "OpenAI CLIP explained",
    "section": "",
    "text": "Pretraining on ImageNet1 dataset and finetuning on the downstream task has been the standard procedure in Computer Vision. Although such methods have shown great performance, they have some major limitations.\nBoth of these limitations can be addressed if the model can use natural language supervision to learn the representations. We can have abundant data in (image, text) pairs and use this data to learn the representations that can be utilized for zero-shot transfer. CLIP2 by OpenAI is one such mechanism for training visual models using natural language. Since it enables zero-shot transfer, we need not be concerned with the number of classes in the downstream task.\nIn this post, we’ll discuss the method used in CLIP. The major achievement of CLIP is that it achieves zero-shot performance that is on par with supervised training."
  },
  {
    "objectID": "posts/openai-clip-explained.html#dataset",
    "href": "posts/openai-clip-explained.html#dataset",
    "title": "OpenAI CLIP explained",
    "section": "Dataset",
    "text": "Dataset\nA new dataset of 400 million pairs of images and text is constructed. All those images contain natural language descriptions which are available over the internet. Manual annotation denoting the class present in the image is not required. Table 1 contains an example of a dataset containing 2 pairs of (image, text) where text is a natural language description of an image. You can imagine the amount of knowledge present in 400 million such pairs.\n\n\n\nTable 1: Dataset Example\n\n\n\n\n\nPair\nImage Description\nImage\n\n\n\n\nPair 1\nA white cat\n\n\n\nPair 2\nA black and white dog"
  },
  {
    "objectID": "posts/openai-clip-explained.html#method",
    "href": "posts/openai-clip-explained.html#method",
    "title": "OpenAI CLIP explained",
    "section": "Method",
    "text": "Method\nFirstly, pretraining is done using contrastive setup and the pretrained model can be used for zero-shot transfer. Here I present an example of classification task. For other tasks and dataset, please refer to the paper2.\n\nPretraining\nThe pretraining phase is shown in Figure 1. It consists of a text encoder and an image encoder. In the paper, they experiment with variants of ResNet3 and ViT4 for image encoder and Transformer5 for text encoder. The motivation is to use natural language to guide vision representation learning. The text encoder processes the descriptions of images and the image encoder processes the images. Text and image representations are obtained from the text and the image encoder respectively. The objective is to make the representations of a pair of image and text close and the representations of image and text across different pairs to be different. Using the data given in Table 1, the pre-training objective would be to make the representation of the text A white cat and the cat's image as similar as possible. Similarly the representation of A black and white dog and the dog's image also should be as similar as possible. However, the representations between text and image of different pairs must be as dissimilar as possible. Contrastive Learning6 can be used to achieve this.\n\nMake the representations of text and image in the same pair to be the same and different between text and image of different pairs.\n\nOnce this objective is achieved, the vision model will learn to produce features that are similar with to textual features. Here natural language is being used for supervision instead of image labels. The vision model pretrained in such a way can be used in zero-shot classification tasks.\n\n\nZero-shot transfer\nZero-shot transfer means using the model on the target dataset without showing any example from that dataset. Zero-shot transfer example is shown in Figure 2. The process can be described in the following steps:\n\nIdentify the classes to which the image should be classified (plane, car, dog…bird). These can be any class. However, you should be able to represent them using natural language.\nAdd prompts to these classes. In Figure 2, a prompt A photo of a is used. Experiments have shown prompts to improve performance compared to contextless class names only.\nPass the image to be classified through the image encoder and prompted class names through the text encoder. Get the representations of images and text.\nCompute the cosine similarities between the representation of images and text. Identify the image of the classes with the highest similarity.\n\nHere, it can be seen how a pre-trained model can be used in any dataset without using any image from the target dataset for finetuning.\n\n\n\n\n\n\nFigure 1: Pretraining Stage\n\n\n\n\n\n\n\n\n\nFigure 2: Inference"
  },
  {
    "objectID": "posts/openai-clip-explained.html#conclusion",
    "href": "posts/openai-clip-explained.html#conclusion",
    "title": "OpenAI CLIP explained",
    "section": "Conclusion",
    "text": "Conclusion\nWe discussed how CLIP enables zero-shot transfer. This is quite a big achievement given it does not use any manual annotation in training and finetuning. Due to the effectiveness of such embeddings, CLIP-based models have been used in guiding Diffusion based models to generate images with natural language descriptions. CLIP is widely used in learning multi-modal embeddings.\nPlease feel free to post comments or open a pull request if you think any correction or addition needs to be made. Thank you for going through the post. I’ll see you in the next one. Bye!"
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html",
    "href": "posts/knowledge-distillation-explained.html",
    "title": "Knowledge Distillation explained",
    "section": "",
    "text": "In this post I’ll be discussing about Knowledge Distillation. Basically, I’ll be summarizing the concept from this paper on Knowledge Distillation. Knowledge Distillation is the process of training a smaller network by using the concepts/knowledge that has been learned by the large network. Basically, it is the process of distilling knowledge from one model to another."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#motivation",
    "href": "posts/knowledge-distillation-explained.html#motivation",
    "title": "Knowledge Distillation explained",
    "section": "Motivation",
    "text": "Motivation\nLarger models have high learning capacity and can perform well when trained on large datasets. This is the pattern followed in Machine Learning. If getting the best accuracy is the only aim then this approach is fine. But when we want to deploy the model in hardware with less compute or time constraints (e.g: deploying models on mobile phones), deploying large models is not an option. We want to have smaller models that perform on par or close to large models but be efficient in computation. The main motivation behind Knowledge Distillation is to use large, complex models during training so that they can extract a lot of concepts from the data and use these models to train smaller models that will be used in inference as they are more efficient (computation and memory-wise) than large models."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#analogy",
    "href": "posts/knowledge-distillation-explained.html#analogy",
    "title": "Knowledge Distillation explained",
    "section": "Analogy",
    "text": "Analogy\nAn analogy from stages of butterfly’s life can be related to the concepts of Knowledge Distillation. They need to eat a lot to gain nutrition and energy. They also need to be light to fly around and mate. Since these are opposite tasks, there are separate forms for such tasks. Caterpillar is the stage that feeds a lot to gain energy and nutrition. Its task is just eating. Once this stage is complete, it is transformed to butterfly for tasks such as flying, mating that require lightweight. The same pattern can be suitable in Machine Learning as discussed in motivation (training a large complicated model and using a small/light model for inference)."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#task",
    "href": "posts/knowledge-distillation-explained.html#task",
    "title": "Knowledge Distillation explained",
    "section": "Task",
    "text": "Task\nThe task in Knowledge Distillation is to train a smaller model that performs better than itself trained from scratch. For this, a large model using lots of data to high accuracy will be trained first. This is known as a teacher. It is able to learn a lot of concepts with its large size/learning capacity. For inference, a smaller model is trained using the knowledge acquired by the large model. The small model that is used in inference is called student."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#knowledge",
    "href": "posts/knowledge-distillation-explained.html#knowledge",
    "title": "Knowledge Distillation explained",
    "section": "Knowledge",
    "text": "Knowledge\n\n\n\n\n\n\n\n\n\nInput image\n\n\n\n\n\n\n\nPredicted class\nCar\nTruck\nCarrot\n\n\nConfidence\n0.9\n0.09\n0.01\n\n\n\nKnowledge is the concept that is acquired by the teacher model, i.e: output of teacher for various images. In the above example, although the model is quite sure that the image is of car, there is something interesting about probabilities of other classes as well. We can see that the model is far more sure about the image being a truck than a carrot. This knowledge is used by Knowledge Distillation to train a smaller model. The model having 0.09 confidence of being a truck and 0.01 confidence of being a carrot is a very useful information. We ignore this knowledge while training a classifier. Knowledge distillation aims to use this knowledge effectively to train the smaller model. It is also called dark knowledge as it exists in the model but is not used for downstream task. Knowledge distillation utilizes this knowledge."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#approach",
    "href": "posts/knowledge-distillation-explained.html#approach",
    "title": "Knowledge Distillation explained",
    "section": "Approach",
    "text": "Approach\n\nSoft targets\n\n\n\n\n\n\n\n\n\nTarget Type (\\(\\downarrow\\)) Class (\\(\\rightarrow\\))\nCar\nTruck\nCarrot\n\n\n\n\nHard targets\n1\n0\n0\n\n\nModel outputs\n0.9\n0.09\n0.01\n\n\nSoft targets\n0.8\n0.15\n0.05\n\n\n\nAs seen in the above table, there are three types of targets that denote the class of a given image. Hard targets denote the class to which the image belongs using 1 and the rest of the classes have a value 0. This is also called one-hot encoding. The second row is the example of model outputs without any changes. In the case of multi-class classification, it is the value of softmax that outputs softer distribution than hard targets and all the classes are assigned some probability values. The last row denotes soft targets which are softened by using the temperature parameter in softmax function. As we make the distribution softer and softer, high probabilities will decrease and small probabilities will increase. Making the distribution softer can make the knowledge valuable to student as it can carry the concepts more clearly (in this case the probabilities of each class i.e: model gives the information not just about the image that it is most sure of but regarding other classes as well). In this example, the model can provide information that there are details that are related to truck and carrot (although smaller) which can be a valuable information while teaching the student model.\n\nSoftmax function\n\\[q_i = \\frac{exp(z_i)}{\\sum_{j}exp(z_j)}\\]\n\n\nSoftmax function with temperature parameter\n\\[q_i = \\frac{exp(z_i/\\tau)}{\\sum_{j}exp(z_j/\\tau)}\\]\n\\(\\tau\\) is called temperature. It controls the extent to which the distribution is to be softened and can be decided by using hyperparameter search.\nIn PyTorch this can be computed as:\nimport torch\nimport torch.nn as nn\n\nlogits = torch.tensor([2.8, 0.1, -1]).reshape(1, -1) #output of model for car, truck and carrot\nt = 1 #temperature parameter to make distribution soft\nsoftmax = nn.Softmax(dim=1)\n\nsoftmax(logits/t)\n\n# Outputs\ntensor([[0.9178, 0.0617, 0.0205]]) # t=1\ntensor([[0.7098, 0.1840, 0.1062]]) # t=2\ntensor([[0.5923, 0.2408, 0.1669]]) # t=3\ntensor([[0.4877, 0.2842, 0.2281]]) # t=5\nAs we increase the temperature parameter the softmax output changes to a softer distribution."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#training-teacher",
    "href": "posts/knowledge-distillation-explained.html#training-teacher",
    "title": "Knowledge Distillation explained",
    "section": "Training teacher",
    "text": "Training teacher\nTraining a teacher is similar to training other neural networks. Since the objective is to learn as much as possible so that the student could be taught using the knowledge learned by the teacher, normally large model is trained on large datasets. Teacher can also be an ensemble of models."
  },
  {
    "objectID": "posts/knowledge-distillation-explained.html#training-student",
    "href": "posts/knowledge-distillation-explained.html#training-student",
    "title": "Knowledge Distillation explained",
    "section": "Training student",
    "text": "Training student\nTraining a student is the main contribution of Knowledge Distillation. Student is trained using the knowledge gathered by teacher as well as the ground truth labels.\n\\[p^\\tau_T = \\text{softmax}(a_T/\\tau)\\]\n\\[p^\\tau_S = \\text{softmax}(a_S/\\tau)\\]\n\\[hard\\_loss = CE(y_{true}, p_S)\\]\n\\[soft\\_loss = KL(p^\\tau_T, p^\\tau_S)\\]\n\\[KD\\_loss = (1 - \\lambda) * soft\\_loss + \\lambda * hard\\_loss\\]\n\\(\\lambda\\) is the weight hyperparameter, \\(CE\\) and \\(KL\\) denote Crossentropy and KL divergence respectively.\nWe can train the student by minimizing KD_loss. As seen from the equation, Knowledge distillation uses both the hard labels and soft labels to train the student model.\n\nAs seen in the diagram above, student tries to match the soft targets from the teacher."
  },
  {
    "objectID": "posts/class-activation-map-explained.html",
    "href": "posts/class-activation-map-explained.html",
    "title": "Class Activation Map",
    "section": "",
    "text": "Code\n# imports\n\nimport ast\nimport json\nfrom copy import deepcopy\n\nimport imageio\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models, transforms"
  },
  {
    "objectID": "posts/class-activation-map-explained.html#pytorch-hooks",
    "href": "posts/class-activation-map-explained.html#pytorch-hooks",
    "title": "Class Activation Map",
    "section": "Pytorch Hooks",
    "text": "Pytorch Hooks\nWe’ll be using PyTorch hooks to extract the intermediate feature maps. Hooks are the functions that can be executed during forward or backward pass of the network. You can learn model about hooks here.\nHere we are using hooks to save the feature maps of layer4, i.e. the final convolutional layer. We multiply the output of layer4 by the weights that map from avgpool layer to fc layer. Since weights can be extracted from the model itself, we only use hooks to save the output of laster convolutional layer.\n\n\nCode\n# define hooks to save activation\nactivation = {}\n\ndef get_activation(name):\n    def hook(model, input, output):        \n        activation[name] = output.detach()\n    return hook\n\n\nOnce we define attach hook to the model, the output of the layer to which the hooks was attached will be saved in the activation dictionary."
  },
  {
    "objectID": "posts/class-activation-map-explained.html#implementation",
    "href": "posts/class-activation-map-explained.html#implementation",
    "title": "Class Activation Map",
    "section": "Implementation",
    "text": "Implementation\n\n\nCode\n# define function to get predictions and required activations\n\ndef get_data(input_image, model, transforms):\n    # transform input image\n    input_data = transform(imageio.imread(input_image)).unsqueeze(0)\n    \n    # imagenet labels to map from index to text label\n    labels = ast.literal_eval(open(\"imagenet1000_clsidx_to_labels.txt\").read())\n    \n    # attach hook to layer 4\n    model.layer4.register_forward_hook(get_activation(\"layer4\"))\n    \n    model.eval();\n    with torch.no_grad():\n        preds = model(input_data)\n        preds_softmax = torch.nn.functional.softmax(preds, dim=1)\n        top_prob, top_pred = preds_softmax.max(dim=1)\n\n        return top_pred, top_prob, labels[top_pred.item()], activation\n\n\n\n\nCode\n# image to test CAM\nimage = \"images/n02102040_7490.jpeg\"\n\n# model to run inference\nmodel = models.resnet50(pretrained=True)\n\n# transforms to use ImageNet values for normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# results\npred_class, pred_prob, pred_label, activation = get_data(image, model, transforms)\n\npred_class, pred_prob, pred_label\n\n\n/Users/pramesh/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/pramesh/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/var/folders/rc/vk070bbs1mqgk3d90c9sdrz80000gn/T/ipykernel_21292/2273823473.py:5: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  input_data = transform(imageio.imread(input_image)).unsqueeze(0)\n\n\n(tensor([217]), tensor([0.8006]), 'English springer, English springer spaniel')\n\n\n\n\nCode\n# input image\ninput_img = imageio.imread(image)\nplt.imshow(input_img);\n\n\n/var/folders/rc/vk070bbs1mqgk3d90c9sdrz80000gn/T/ipykernel_21292/1954344181.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  input_img = imageio.imread(image)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# extract weights from FC layer that correspond to the predicted class\n\nfc_weights = model.fc.weight[pred_class, :].unsqueeze(2).unsqueeze(3)\nfc_weights.shape\n\n\ntorch.Size([1, 2048, 1, 1])\n\n\n\n\nCode\n# check shape of final convolutional layer\nactivation[\"layer4\"].shape\n\n\ntorch.Size([1, 2048, 10, 14])\n\n\n\n\nCode\n# weighted summation of final feature maps\ncam = (fc_weights * activation[\"layer4\"]).sum(1, keepdim=True)\n\n# add dimension to make compatible with interpolation\ncam = cam\ncam.shape\n\n\ntorch.Size([1, 1, 10, 14])\n\n\n\n\nCode\n# reshape cam to original input shape\nfinal_cam = F.interpolate(cam, input_img.shape[:2], mode=\"bilinear\", align_corners=True)\n\n\n\n\nCode\n# plot input image with CAM overlayed\nplt.imshow(input_img)\nplt.imshow(final_cam.squeeze().detach().numpy(), alpha=0.8)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs can be seen in the plot above, the network is able to focus on the dogs while classification. We can weight the features first and then resize the image or we can resize the features first and use the weighted combination. The results are identical. Below I have first resized the final feature maps and then used their weighted combination for plotting.\n\n\nCode\n# resizing the feature maps and then weighting\nfeatures = F.interpolate(activation[\"layer4\"], tuple(input_img.shape[:2]), mode=\"bilinear\", align_corners=True)\n\ncam_new = (fc_weights * features).sum(1, keepdim=True)\ncam_new.shape\n\n\ntorch.Size([1, 1, 320, 426])\n\n\n\n\nCode\nplt.imshow(input_img)\nplt.imshow(cam_new.squeeze().detach().numpy(), alpha=0.8);\n\n\n\n\n\n\n\n\n\nThank you for going through the post and I hope is was helpful to understand Class Activation Maps. I will add Grad-CAM in future post. Please post comments if anything is not clear or if you have any suggestions.\nBye :)"
  },
  {
    "objectID": "posts/barlow-twins-explanation.html",
    "href": "posts/barlow-twins-explanation.html",
    "title": "Meta AI Barlow Twins paper Explained",
    "section": "",
    "text": "Self-supervised learning (SSL) is the idea of learning representations from data without the use of any data labels. Supervised algorithms in vision normally use Imagenet data where the image labels are used as supervision while training the model. On the other hand, self-supervised approaches are designed in such a way that ground truth labels are not required. This eliminates a lot of careful annotation that needs to be performed by humans. This in turn enables the use of vast amounts of unlabeled data. Normally self-supervised approaches in Computer Vision are based on aligning the features of two different augmentations of a single image.\nThe general architecture of self-supervised learning is shown in Figure 1. Here \\(T\\) is a set of different augmentations that can be composed. \\(t^A\\) and \\(t^B\\) are sampled from \\(T\\) and image \\(X\\) is transformed using these augmentations. Augmented images are fed to the same neural network \\(f\\) which generates features \\(Z^A\\) and \\(Z^B\\) for transformed images \\(X^A\\) and \\(X^B\\) respectively. \\(f\\) can be a shared network or unshared. The objective of SSL is to make \\(Z^A\\) and \\(Z^B\\) similar for pairs of the same image and different across different images. Self-supervised learning is based on this idea.\nOne thing to be careful of in SSL to prevent collapse i.e the situation where \\(Z^A\\) and \\(Z^B\\) are the same for every image. There are various techniques to prevent collapse. Contrastive learning and asymmetric updates are some of those approaches. For a review of self-supervised methods, you can refer to1."
  },
  {
    "objectID": "posts/barlow-twins-explanation.html#motivation",
    "href": "posts/barlow-twins-explanation.html#motivation",
    "title": "Meta AI Barlow Twins paper Explained",
    "section": "Motivation",
    "text": "Motivation\nBarlow Twins is a method of learning representations without using supervised data i.e. it is a self-supervised learning approach. It tries to overcome the design decisions that were required to prevent collapse in previous SSL methods. Some of those techniques are the use of negative samples, asymmetric learning updates, stop-gradient operation, large batch size, etc. This paper eliminates the need for such choices and achieves results comparable with state-of-the-art methods. This is based on an idea called redundancy reduction which has been borrowed from neuroscience.\nBefore going into the methods, let’s understand the different types of collapse that can occur. This paper avoids the collapse situations mentioned below.\n\nThe first collapse scenario is the one where the model outputs the same features for all the images. In doing so, the features will be the same across augmentations for a single image (which is required) but it will also be the same for all the images (which is not required). One common solution to avoiding this type of collapse is to use negative samples and make the representations of positive samples (views of the same image) closer in feature space and the representations of negative samples (views of other images) far apart. Contrastive learning3 is probably the most famous method to avoid this form of collapse. This form of collapse is shown in Figure 2. In the figure, both the cat and dog images have the same features.\n\n\n\n\n\n\n\nFigure 2: Collapse scenario having same features for all images\n\n\n\n\nThe other form of collapse (not exactly a collapse but redundancy) is shown in Figure 3. In this scenario, the features are different across different images and the same for the different views of the same image. This is what we want while learning representations. But the problem is that the features are redundant i.e repetition across multiple dimensions. We want to avoid this case since this adds redundancy and learns nothing significant. In Figure 3, both views of the cat have the same features (required case) but the features have duplicate entries which add redundancy and learn nothing significant.\n\n\n\n\n\n\n\nFigure 3: Collapse scenario having redundant features\n\n\n\n\n\n\n\n\n\nFigure 4: Required scenario with redundancy removed\n\n\n\nWe want the features for different images to be different but the features for different views of the same image to be the same. On top of that, we want the features to avoid redundancy. This is shown in Figure 4. Here the features for the rotated views of the cat’s images are the same but the features in different positions are different i.e. redundancy is avoided. The loss function introduced in Barlow Twins achieves this and we discuss this below."
  },
  {
    "objectID": "posts/barlow-twins-explanation.html#architecture",
    "href": "posts/barlow-twins-explanation.html#architecture",
    "title": "Meta AI Barlow Twins paper Explained",
    "section": "Architecture",
    "text": "Architecture\n\n\n\n\n\n\nFigure 5: Barlow Twins Architecture\n\n\n\nThe architecture of Barlow Twins is shown in Figure 5. The objective is to make the features of different augmentations similar with the removal of redundancy and this is achieved by trying to make the cross-correlation matrix of features approach identity. We’ll discuss this in detail in Loss Function."
  },
  {
    "objectID": "posts/barlow-twins-explanation.html#loss-function",
    "href": "posts/barlow-twins-explanation.html#loss-function",
    "title": "Meta AI Barlow Twins paper Explained",
    "section": "Loss Function",
    "text": "Loss Function\n\\[\nL_{BT} \\triangleq \\sum_i(1-C_{ii})^2 + \\lambda \\sum_i \\sum_{j \\neq i} C_{ij}^2\n\\tag{1}\\]\n\\[\nC_{ij} \\triangleq \\frac{\\sum_b z_{b,i}^A z_{b,j}^B}{\\sqrt{\\sum_b (z_{b, i}^A)^2} \\sqrt{\\sum_b (z_{b, j}^B)^2}}\n\\tag{2}\\]\nThe loss function used in Barlow Twins is shown in Equation 1. The first part is called invariance term and the second term is called redundancy reduction term. The idea is to make the cross-correlation matrix of features of two views of images close to the identity matrix. When the cross-correlation matrix is close to identity it will cause the features in the same position to be similar and the features in different positions to be different from one another. This is captured in the cross-correlation matrix as the matrix is square and the diagonal elements correspond to the features in the same position and the off-diagonal elements are the correlation between features from different positions.\nGoing back to Figure 4, the diagonal elements will be 1 if the features in the same position are the same across two feature vectors and the off-diagonal elements will approach 0 as the features from different positions will become different from one another. i.e \\(C_{12}\\) will be close to 0 when element 1 from the first view’s features and element 2 from the second view’s features are different from one another. This removes redundancy.\nStill, there is another form of collapse to remove where features are the same for different images as shown in Figure 3. It can be removed by normalizing the features. This is shown below where features are transformed to 0 when they are the same for different images. If they are 0s then the loss function will get large and the features need to be made different to minimize the loss. The features being 0 are shown below.\n\nimport torch\nimport torch.nn as nn\n\nbn = nn.BatchNorm1d(4, affine=False)\n\nt = torch.tensor(2*[1., 2., 3., 4.]).reshape(2,4)\nbn(t)\n\ntensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/augmentations-visually-explained.html",
    "href": "posts/augmentations-visually-explained.html",
    "title": "Augmentations: Cutout, Mixup, Cutmix, Label Smoothing",
    "section": "",
    "text": "Data augmentation is one of the major techniques used when we want to improve the model’s performance on new data (i.e. generalization). It is especially useful in cases where only limited training data are available. In this post we’ll discuss some of the augmentation techniques (besides affine transformation) that help the models to perform well (in computer vision).\n\n\nCode\n# install required libraries\n# !pip install opencv-python==4.5.1.48\n# !pip install numpy==1.21.0\n# !pip install matplotlib==3.4.2\n\n\n\n\nCode\n# import the required libraries\nfrom copy import deepcopy\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# set random seed to reproduce results\nnp.random.seed(42)\n\n\n\n\nCode\n# define labels and read images\n\ncat_label = np.array([[1, 0]])\ndog_label = np.array([[0, 1]])\n\ncat_img = cv2.imread(\"images/cat.jpg\")/255.0\ndog_img = cv2.imread(\"images/dog.jpg\")/255.0\n\n\n\n\nCode\n# plot the original images\n\nfig, ((ax1, ax2)) = plt.subplots(1, 2)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nplt.suptitle(\"Original images\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCutout\npaper link\nThe main motivation of Cutout is to simulate the situation of object occlusion that is mostly encountered in tasks such as object recognition or human pose estimation. We occlude the part of image randomly. Instead of model seeing the same image everytime, it sees different parts of that image which helps it to perform well. The occluded part does not contain any information. In the example below, the randomly occluded part is replaced by all 0s.\n\n\nCode\nclass Cutout(object):\n    def __init__(self, cutout_width):\n        # width of the square to occlude\n        self.cutout_width = cutout_width\n\n    def __call__(self, img):\n        h = img.shape[0]\n        w = img.shape[1]\n\n        mask = np.ones(img.shape)\n\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        x1 = np.clip(x - self.cutout_width // 2, 0, w)\n        x2 = np.clip(x + self.cutout_width // 2, 0, w)\n\n        y1 = np.clip(y - self.cutout_width // 2, 0, h)\n        y2 = np.clip(y + self.cutout_width // 2, 0, h)\n\n        mask[y1: y2, x1: x2] = 0.0\n\n        # occlude the part of image using mask (zero out the part)\n        img = img * mask\n\n        return img\n\n\n\n\nCode\ncutout = Cutout(50)\nplt.imshow(cutout(cat_img))\nplt.suptitle(\"Cutout example\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMixup\npaper link\nMixup linearly mixes two images and their labels in the same proportion. With this approach the model will be able to see a different image with different label during training. It enables the model to make smooth decision boundaries while classifying the object since we are using linearly interpolated images and labels for classification decision intead of binary decision. As can be seen in the example below, with the mixup of images and labels, the new image will have the labels [0.19, 0.81] which means the class distribution is tilted more towards dog and the mixup visualization also proves this.\n\n\nCode\nclass Mixup:\n    def __init__(self, img1, img2, label1, label2):\n        self.img1 = img1\n        self.img2 = img2\n        self.label1 = label1\n        self.label2 = label2\n    \n    def __call__(self):\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        \n        # mix image and labels        \n        mix_img = lam * self.img1 + (1 - lam) * self.img2\n        mix_label = lam * self.label1 + (1 - lam) * self.label2\n        \n        return mix_img, mix_label\n\n\n\n\nCode\nmix_img, mix_label = Mixup(cat_img, dog_img, cat_label, dog_label)()\n\n\n\n\nCode\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(mix_img)\nax3.set_title(\"Cat/Dog Mixup\")\nplt.suptitle(\"Mixup example\",fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCutmix\npaper link\nAs we saw above, the occluded part in Cutout does not contain any information which in unwanted since it just increases resource usage without adding any value. Cutmix utilizes above two techniques to mix the images and utilize the occluded part. As can be seen in the example, the cutout portion in dog image is replaced by the same portion of cat image. It also linearly interpolates the label as in Mixup. As seen in the example below, the new labels are [0.18, 0.82] for cat and dog respectively and this can be verified from image as well.\n\n\nCode\nclass Cutmix:\n    def __init__(self, img1, img2, lbl1, lbl2):\n        self.img1 = img1\n        self.img2 = img2\n        self.lbl1 = lbl1\n        self.lbl2 = lbl2\n    \n    def __call__(self):\n        # sample bounding box B\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        h, w, c = self.img1.shape\n        \n        r_x = np.random.randint(0, w)\n        r_y = np.random.randint(0, h)\n        r_w = np.int32(w * np.sqrt(1 - lam))\n        r_h = np.int32(h * np.sqrt(1 - lam))\n        \n        mid_x, mid_y = r_w//2, r_h//2\n        \n        point1_x = np.clip(r_x - mid_x, 0, w)\n        point2_x = np.clip(r_x + mid_x, 0, w)\n        \n        point1_y = np.clip(r_y - mid_y, 0, h)\n        point2_y = np.clip(r_y + mid_y, 0, h)\n        \n        # refer to paper to see how M is cimputed, in short it is to make the ratio of mixing images and labels same\n        M = np.ones(self.img1.shape)\n        M[point1_x:point2_x, point1_y:point2_y, :] = 0\n        \n        cut_mix_img = M * self.img1 + (1 - M) * self.img2\n        cut_mix_label = lam * self.lbl1 + (1 - lam) * self.lbl2\n        \n        return cut_mix_img, cut_mix_label\n\n\n\n\nCode\ncut_mix_img, cut_mix_lbl = Cutmix(cat_img, dog_img, cat_label, dog_label)()\n\n\n\n\nCode\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(cut_mix_img)\nax3.set_title(\"Cat mixed onto Dog\")\nplt.suptitle(\"Cutmix example\", fontsize=15)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nLabel Smoothing\npaper link\nLabel smoothing is used so that the model can be prevented from memorizing the training data and being over-confident. It adds noise to the labels without modifying the data itself. If we are using a 5-class classifier then instead of label being assigned only to one class, it is distributed among all the classes. As seen in example below, \\(\\epsilon\\) is used to control the smoothing.\n\n\nCode\nϵ = 0.01\nnum_labels = 5\nclasses = [\"cat\", \"dog\", \"horse\", \"bus\", \"carrot\"]\n\noriginal_label = [1, 0, 0, 0, 0]\nnew_neg_labels = ϵ/(num_labels-1)\n\n# after label smoothing, cat gets 1 - ϵ and other classes get ϵ/(1-ϵ) probability\nsmooth_labels = [1 - ϵ, new_neg_labels, new_neg_labels, new_neg_labels, new_neg_labels]\n\nsmooth_labels\n\n\n[0.99, 0.0025, 0.0025, 0.0025, 0.0025]\n\n\nAll the above implementations assume that both the images are of same resolution. There might be some minor differences while compared with the original paper. However the main motivation and results of these techniques are same to that of paper. Please feel free to post any comment, question or suggestion. I’ll see you in the next one. :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog by Pramesh Gautam",
    "section": "",
    "text": "From PoC to Production: Lessons Learned Building GenAI Apps\n\n\n\nGenAI\n\n\nEngineering\n\n\nLLM\n\n\n\nSharing my experience of writing LLM-based apps\n\n\n\nDec 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI CLIP explained\n\n\n\ncomputer-vision\n\n\nnlp\n\n\nmulti-modal\n\n\n\nLearn how contrastive learning connects vision and language for zero-shot image classification\n\n\n\nApr 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta AI Barlow Twins paper Explained\n\n\n\ncomputer-vision\n\n\nself-supervised-learning\n\n\n\nLearn how redundancy reduction enables learning without labels in computer vision.\n\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Activation Map\n\n\n\ncomputer-vision\n\n\n\nVisualize what neural networks see with Class Activation Maps (CAM) using ResNet and PyTorch.\n\n\n\nAug 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugmentations: Cutout, Mixup, Cutmix, Label Smoothing\n\n\n\ncomputer-vision\n\n\naugmentations\n\n\n\nLearn data augmentation techniques for computer vision with visual explanations and Python implementations.\n\n\n\nMar 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation explained\n\n\n\ncomputer-vision\n\n\ndistillation\n\n\n\nLearn knowledge distillation for model compression and efficient deployment.\n\n\n\nOct 3, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  }
]