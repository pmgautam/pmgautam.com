{
  "hash": "97c762aa19ed0e89ed51f23dadeae5ef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Class Activation Map\"\nfreeze: true\naliases: [\"/computer-vision/2022/08/05/Class-activation-maps.html\"]\ndescription: \"Visual explanation to Class Activation Map using ResNet-50 in PyTorch\"\nexecute:\n  echo: true\n  enabled: true\nauthor: Pramesh Gautam\ndate: \"2022-08-05\"\nformat:\n   html:\n      code-fold: true\ntoc: true\nimage: images/cam.png\ncategories: [computer-vision]\n---\n\n::: {#cell-1 .cell execution_count=1}\n``` {.python .cell-code}\n# imports\n\nimport ast\nimport json\nfrom copy import deepcopy\n\nimport imageio\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torchvision import models, transforms\n```\n:::\n\n\n# CAM\n\nClass activation map was introduced in [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150). It was introduced to use the classifier networks for localization tasks. However it can also be used to interpret the models and figure out where the network focuses to classify a input. It uses the weights in the final layer to weight the feature maps in the final convolution layer. That weighted sum is used to see the activation map.\n\n![Class activation map computation](https://github.com/pmgautam/personal-blog/blob/master/images/CAM.png?raw=true)\n\nAs seen in the figure above, once the input image passes through the CONV layers, let's say it produces feature map  of shape $1\\times2048\\times7\\times7$ for the image of input size $1\\times3\\times224\\times224$ in the format $B \\times C \\times H \\times W$ format. Global Average Pooling (GAP) will then sum the spatial dimension and produce output of shape $1\\times2048$ after collapsing across spatial dimension. There will be $2048\\times1000$ weights mapping from output of GAP layer to final layer (1000 number of classes in ImageNet). Once we classify the image, the corresponding 2048 weights mapping from GAP layer to FC layer are used to weight the 2048 spatial features of the last convolutional layer. These weighted features are then resized to the input image size to get the final feature map.\n\nIn the figure, the model predicts the input image as *Australian terrier*. Weights $W_1...W_n$ are the weights that connect the output of GAP layer to the FC layer corresponding to the predicted class. We can say that the weights $W_1...W_n$ are responsible to find the importance of each unit in GAP layer that leads to *Australian terrier* class in output. We use these weights to find the weighted combination of 2048 spatial features in final convolutional layer.\n\n## Pytorch Hooks\nWe'll be using PyTorch hooks to extract the intermediate feature maps. Hooks are the functions that can be executed during forward or backward pass of the network. You can learn model about hooks [here](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/).\n\nHere we are using hooks to save the feature maps of **layer4**, i.e. the final convolutional layer. We multiply the output of **layer4** by the weights that map from avgpool layer to fc layer. Since weights can be extracted from the model itself, we only use hooks to save the output of laster convolutional layer.\n\n::: {#cell-4 .cell execution_count=2}\n``` {.python .cell-code}\n# define hooks to save activation\nactivation = {}\n\ndef get_activation(name):\n    def hook(model, input, output):        \n        activation[name] = output.detach()\n    return hook\n```\n:::\n\n\nOnce we define attach hook to the model, the output of the layer to which the hooks was attached will be saved in the **activation** dictionary.\n\n## Implementation\n\n::: {#cell-7 .cell execution_count=3}\n``` {.python .cell-code}\n# define function to get predictions and required activations\n\ndef get_data(input_image, model, transforms):\n    # transform input image\n    input_data = transform(imageio.imread(input_image)).unsqueeze(0)\n    \n    # imagenet labels to map from index to text label\n    labels = ast.literal_eval(open(\"imagenet1000_clsidx_to_labels.txt\").read())\n    \n    # attach hook to layer 4\n    model.layer4.register_forward_hook(get_activation(\"layer4\"))\n    \n    model.eval();\n    with torch.no_grad():\n        preds = model(input_data)\n        preds_softmax = torch.nn.functional.softmax(preds, dim=1)\n        top_prob, top_pred = preds_softmax.max(dim=1)\n\n        return top_pred, top_prob, labels[top_pred.item()], activation\n```\n:::\n\n\n::: {#cell-8 .cell execution_count=4}\n``` {.python .cell-code}\n# image to test CAM\nimage = \"images/n02102040_7490.jpeg\"\n\n# model to run inference\nmodel = models.resnet50(pretrained=True)\n\n# transforms to use ImageNet values for normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# results\npred_class, pred_prob, pred_label, activation = get_data(image, model, transforms)\n\npred_class, pred_prob, pred_label\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/pramesh/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/pramesh/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/pramesh/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n/tmp/ipykernel_112565/2273823473.py:5: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  input_data = transform(imageio.imread(input_image)).unsqueeze(0)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"e61d42937f5d402e9d69ebe5b810725e\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(tensor([217]), tensor([0.8006]), 'English springer, English springer spaniel')\n```\n:::\n:::\n\n\n::: {#cell-9 .cell execution_count=5}\n``` {.python .cell-code}\n# input image\ninput_img = imageio.imread(image)\nplt.imshow(input_img);\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_112565/1954344181.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  input_img = imageio.imread(image)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](class-activation-map-explained_files/figure-html/cell-6-output-2.png){width=546 height=416}\n:::\n:::\n\n\n::: {#cell-10 .cell execution_count=6}\n``` {.python .cell-code}\n# extract weights from FC layer that correspond to the predicted class\n\nfc_weights = model.fc.weight[pred_class, :].unsqueeze(2).unsqueeze(3)\nfc_weights.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\ntorch.Size([1, 2048, 1, 1])\n```\n:::\n:::\n\n\n::: {#cell-11 .cell execution_count=7}\n``` {.python .cell-code}\n# check shape of final convolutional layer\nactivation[\"layer4\"].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\ntorch.Size([1, 2048, 10, 14])\n```\n:::\n:::\n\n\n::: {#cell-12 .cell execution_count=8}\n``` {.python .cell-code}\n# weighted summation of final feature maps\ncam = (fc_weights * activation[\"layer4\"]).sum(1, keepdim=True)\n\n# add dimension to make compatible with interpolation\ncam = cam\ncam.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\ntorch.Size([1, 1, 10, 14])\n```\n:::\n:::\n\n\n::: {#cell-13 .cell execution_count=9}\n``` {.python .cell-code}\n# reshape cam to original input shape\nfinal_cam = F.interpolate(cam, input_img.shape[:2], mode=\"bilinear\", align_corners=True)\n```\n:::\n\n\n::: {#cell-14 .cell execution_count=10}\n``` {.python .cell-code}\n# plot input image with CAM overlayed\nplt.imshow(input_img)\nplt.imshow(final_cam.squeeze().detach().numpy(), alpha=0.8)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](class-activation-map-explained_files/figure-html/cell-11-output-1.png){width=546 height=416}\n:::\n:::\n\n\nAs can be seen in the plot above, the network is able to focus on the dogs while classification. We can weight the features first and then resize the image or we can resize the features first and use the weighted combination. The results are identical. Below I have first resized the final feature maps and then used their weighted combination for plotting.\n\n::: {#cell-16 .cell execution_count=11}\n``` {.python .cell-code}\n# resizing the feature maps and then weighting\nfeatures = F.interpolate(activation[\"layer4\"], tuple(input_img.shape[:2]), mode=\"bilinear\", align_corners=True)\n\ncam_new = (fc_weights * features).sum(1, keepdim=True)\ncam_new.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\ntorch.Size([1, 1, 320, 426])\n```\n:::\n:::\n\n\n::: {#cell-17 .cell execution_count=12}\n``` {.python .cell-code}\nplt.imshow(input_img)\nplt.imshow(cam_new.squeeze().detach().numpy(), alpha=0.8);\n```\n\n::: {.cell-output .cell-output-display}\n![](class-activation-map-explained_files/figure-html/cell-13-output-1.png){width=546 height=416}\n:::\n:::\n\n\nThank you for going through the post and I hope is was helpful to understand Class Activation Maps. I will add Grad-CAM in future post. Please post comments if anything is not clear or if you have any suggestions.\n\nBye :)\n\n# References\n1. [https://arxiv.org/abs/1512.04150](https://arxiv.org/abs/1512.04150)\n\n\n",
    "supporting": [
      "class-activation-map-explained_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"0ebb7c526af04ed7b0169958be60338b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"4bd9122fafef4929beb6015b768a9bbf\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6bad7dc514734a3fadb14acdf8f59801\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"8fe4d0efd8b344088c73120ea94019bc\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"a55c4544ba02457fa9c902c28be62f03\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_a80d322343a444e3980178f6eae5ea0a\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_6bad7dc514734a3fadb14acdf8f59801\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 97.8M/97.8M [00:12&lt;00:00, 8.56MB/s]\"}},\"a80d322343a444e3980178f6eae5ea0a\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"be1fd7f2a96a463a908769c1382ccc4b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"c52ac92f3e324386920be3286a3948e0\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_be1fd7f2a96a463a908769c1382ccc4b\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_c67553ffa9fc4ab0b4ce3b4153a939a5\",\"tabbable\":null,\"tooltip\":null,\"value\":\"100%\"}},\"c55344a6324f4df8a80ab45d6b57fd4a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_4bd9122fafef4929beb6015b768a9bbf\",\"max\":102530333,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_8fe4d0efd8b344088c73120ea94019bc\",\"tabbable\":null,\"tooltip\":null,\"value\":102530333}},\"c67553ffa9fc4ab0b4ce3b4153a939a5\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"e61d42937f5d402e9d69ebe5b810725e\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_c52ac92f3e324386920be3286a3948e0\",\"IPY_MODEL_c55344a6324f4df8a80ab45d6b57fd4a\",\"IPY_MODEL_a55c4544ba02457fa9c902c28be62f03\"],\"layout\":\"IPY_MODEL_0ebb7c526af04ed7b0169958be60338b\",\"tabbable\":null,\"tooltip\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}