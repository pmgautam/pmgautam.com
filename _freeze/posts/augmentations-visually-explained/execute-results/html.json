{
  "hash": "7cb408b4af0667611bd07ba0db170c7d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Augmentations: Cutout, Mixup, Cutmix, Label Smoothing\"\naliases: [\"/augmentations/2022/03/27/Augmentations-visually-explained.html\"]\ndescription: \"Visual summary and implementation of augmentations techniques: Cutout, Mixup, Cutmix and Label Smoothing\"\nexecute: \n  echo: true\n  enabled: true\nauthor: Pramesh Gautam\ndate: \"2022-03-27\"\nformat:\n   html:\n      code-fold: true\ntoc: true\nimage: images/aug.png\ncategories: [computer-vision, augmentations]\n---\n\nData augmentation is one of the major techniques used when we want to improve the model's performance on new data (i.e. generalization). It is especially useful in cases where only limited training data are available. In this post we'll discuss some of the augmentation techniques (besides affine transformation) that help the models to perform well (in computer vision).\n\n::: {#cell-2 .cell execution_count=1}\n``` {.python .cell-code}\n# install required libraries\n# !pip install opencv-python==4.5.1.48\n# !pip install numpy==1.21.0\n# !pip install matplotlib==3.4.2\n```\n:::\n\n\n::: {#cell-3 .cell execution_count=2}\n``` {.python .cell-code}\n# import the required libraries\nfrom copy import deepcopy\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# set random seed to reproduce results\nnp.random.seed(42)\n```\n:::\n\n\n::: {#cell-4 .cell execution_count=3}\n``` {.python .cell-code}\n# define labels and read images\n\ncat_label = np.array([[1, 0]])\ndog_label = np.array([[0, 1]])\n\ncat_img = cv2.imread(\"images/cat.jpg\")/255.0\ndog_img = cv2.imread(\"images/dog.jpg\")/255.0\n```\n:::\n\n\n::: {#cell-5 .cell execution_count=4}\n``` {.python .cell-code}\n# plot the original images\n\nfig, ((ax1, ax2)) = plt.subplots(1, 2)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nplt.suptitle(\"Original images\",fontsize=15)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](augmentations-visually-explained_files/figure-html/cell-5-output-1.png){width=575 height=393}\n:::\n:::\n\n\n# Cutout\n\n[paper link](https://arxiv.org/abs/1708.04552)\n\nThe main motivation of Cutout is to simulate the situation of object occlusion that is mostly encountered in tasks such as object recognition or human pose estimation. We occlude the part of image randomly. Instead of model seeing the same image everytime, it sees different parts of that image which helps it to perform well. The occluded part does not contain any information. In the example below, the randomly occluded part is replaced by all 0s.\n\n::: {#cell-8 .cell execution_count=5}\n``` {.python .cell-code}\nclass Cutout(object):\n    def __init__(self, cutout_width):\n        # width of the square to occlude\n        self.cutout_width = cutout_width\n\n    def __call__(self, img):\n        h = img.shape[0]\n        w = img.shape[1]\n\n        mask = np.ones(img.shape)\n\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        x1 = np.clip(x - self.cutout_width // 2, 0, w)\n        x2 = np.clip(x + self.cutout_width // 2, 0, w)\n\n        y1 = np.clip(y - self.cutout_width // 2, 0, h)\n        y2 = np.clip(y + self.cutout_width // 2, 0, h)\n\n        mask[y1: y2, x1: x2] = 0.0\n\n        # occlude the part of image using mask (zero out the part)\n        img = img * mask\n\n        return img\n```\n:::\n\n\n::: {#cell-9 .cell execution_count=6}\n``` {.python .cell-code}\ncutout = Cutout(50)\nplt.imshow(cutout(cat_img))\nplt.suptitle(\"Cutout example\",fontsize=15)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](augmentations-visually-explained_files/figure-html/cell-7-output-1.png){width=430 height=459}\n:::\n:::\n\n\n# Mixup\n[paper link](https://arxiv.org/abs/1710.09412)\n\nMixup linearly mixes two images and their labels in the same proportion. With this approach the model will be able to see a different image with different label during training. It enables the model to make smooth decision boundaries while classifying the object since we are using linearly interpolated images and labels for classification decision intead of binary decision. As can be seen in the example below, with the mixup of images and labels, the new image will have the labels `[0.19, 0.81]` which means the class distribution is tilted more towards dog and the mixup visualization also proves this.\n\n::: {#cell-11 .cell execution_count=7}\n``` {.python .cell-code}\nclass Mixup:\n    def __init__(self, img1, img2, label1, label2):\n        self.img1 = img1\n        self.img2 = img2\n        self.label1 = label1\n        self.label2 = label2\n    \n    def __call__(self):\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        \n        # mix image and labels        \n        mix_img = lam * self.img1 + (1 - lam) * self.img2\n        mix_label = lam * self.label1 + (1 - lam) * self.label2\n        \n        return mix_img, mix_label\n```\n:::\n\n\n::: {#cell-12 .cell execution_count=8}\n``` {.python .cell-code}\nmix_img, mix_label = Mixup(cat_img, dog_img, cat_label, dog_label)()\n```\n:::\n\n\n::: {#cell-13 .cell execution_count=9}\n``` {.python .cell-code}\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(mix_img)\nax3.set_title(\"Cat/Dog Mixup\")\nplt.suptitle(\"Mixup example\",fontsize=15)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](augmentations-visually-explained_files/figure-html/cell-10-output-1.png){width=575 height=351}\n:::\n:::\n\n\n# Cutmix\n[paper link](https://arxiv.org/abs/1905.04899)\n\nAs we saw above, the occluded part in Cutout does not contain any information which in unwanted since it just increases resource usage without adding any value. Cutmix utilizes above two techniques to mix the images and utilize the occluded part. As can be seen in the example, the cutout portion in dog image is replaced by the same portion of cat image. It also linearly interpolates the label as in Mixup. As seen in the example below, the new labels are `[0.18, 0.82]` for cat and dog respectively and this can be verified from image as well.\n\n::: {#cell-15 .cell execution_count=10}\n``` {.python .cell-code}\nclass Cutmix:\n    def __init__(self, img1, img2, lbl1, lbl2):\n        self.img1 = img1\n        self.img2 = img2\n        self.lbl1 = lbl1\n        self.lbl2 = lbl2\n    \n    def __call__(self):\n        # sample bounding box B\n        alpha = 1\n        lam = np.random.beta(alpha, alpha)\n        h, w, c = self.img1.shape\n        \n        r_x = np.random.randint(0, w)\n        r_y = np.random.randint(0, h)\n        r_w = np.int32(w * np.sqrt(1 - lam))\n        r_h = np.int32(h * np.sqrt(1 - lam))\n        \n        mid_x, mid_y = r_w//2, r_h//2\n        \n        point1_x = np.clip(r_x - mid_x, 0, w)\n        point2_x = np.clip(r_x + mid_x, 0, w)\n        \n        point1_y = np.clip(r_y - mid_y, 0, h)\n        point2_y = np.clip(r_y + mid_y, 0, h)\n        \n        # refer to paper to see how M is cimputed, in short it is to make the ratio of mixing images and labels same\n        M = np.ones(self.img1.shape)\n        M[point1_x:point2_x, point1_y:point2_y, :] = 0\n        \n        cut_mix_img = M * self.img1 + (1 - M) * self.img2\n        cut_mix_label = lam * self.lbl1 + (1 - lam) * self.lbl2\n        \n        return cut_mix_img, cut_mix_label\n```\n:::\n\n\n::: {#cell-16 .cell execution_count=11}\n``` {.python .cell-code}\ncut_mix_img, cut_mix_lbl = Cutmix(cat_img, dog_img, cat_label, dog_label)()\n```\n:::\n\n\n::: {#cell-17 .cell execution_count=12}\n``` {.python .cell-code}\nfig, ((ax1, ax2, ax3)) = plt.subplots(1, 3)\n\nax1.imshow(cat_img)\nax1.set_title(\"Cat\")\n\nax2.imshow(dog_img)\nax2.set_title(\"Dog\")\n\nax3.imshow(cut_mix_img)\nax3.set_title(\"Cat mixed onto Dog\")\nplt.suptitle(\"Cutmix example\", fontsize=15)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](augmentations-visually-explained_files/figure-html/cell-13-output-1.png){width=578 height=351}\n:::\n:::\n\n\n# Label Smoothing\n\n[paper link](https://arxiv.org/abs/1512.00567)\n\nLabel smoothing is used so that the model can be prevented from memorizing the training data and being over-confident. It adds noise to the labels without modifying the data itself. If we are using a 5-class classifier then instead of label being assigned only to one class, it is distributed among all the classes. As seen in example below, $\\epsilon$ is used to control the smoothing.\n\n::: {#cell-20 .cell execution_count=13}\n``` {.python .cell-code}\nϵ = 0.01\nnum_labels = 5\nclasses = [\"cat\", \"dog\", \"horse\", \"bus\", \"carrot\"]\n\noriginal_label = [1, 0, 0, 0, 0]\nnew_neg_labels = ϵ/(num_labels-1)\n\n# after label smoothing, cat gets 1 - ϵ and other classes get ϵ/(1-ϵ) probability\nsmooth_labels = [1 - ϵ, new_neg_labels, new_neg_labels, new_neg_labels, new_neg_labels]\n\nsmooth_labels\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[0.99, 0.0025, 0.0025, 0.0025, 0.0025]\n```\n:::\n:::\n\n\nAll the above implementations assume that both the images are of same resolution. There might be some minor differences while compared with the original paper. However the main motivation and results of these techniques are same to that of paper. Please feel free to post any comment, question or suggestion. I'll see you in the next one. :)\n\n\n",
    "supporting": [
      "augmentations-visually-explained_files"
    ],
    "filters": [],
    "includes": {}
  }
}